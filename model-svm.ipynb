{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport nltk\n\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('treebank')\nfrom nltk.chunk import tree2conlltags\nfrom nltk.corpus import names\nfrom nltk import word_tokenize\nimport re\nimport csv \nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport spacy\nimport en_core_web_sm\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\nfrom nltk import pos_tag\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import wordnet as wn\nfrom sklearn import model_selection, svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns; sns.set_theme()\nfrom sklearn import preprocessing\nfrom sklearn.metrics import classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-05T16:28:21.348477Z","iopub.execute_input":"2021-10-05T16:28:21.348929Z","iopub.status.idle":"2021-10-05T16:28:21.362038Z","shell.execute_reply.started":"2021-10-05T16:28:21.348885Z","shell.execute_reply":"2021-10-05T16:28:21.361239Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# loading test and training data","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(r\"/kaggle/input/nlp-getting-started/train.csv\",encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.364021Z","iopub.execute_input":"2021-10-05T16:28:21.364611Z","iopub.status.idle":"2021-10-05T16:28:21.405220Z","shell.execute_reply.started":"2021-10-05T16:28:21.364566Z","shell.execute_reply":"2021-10-05T16:28:21.403983Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"df_test=pd.read_csv(r\"/kaggle/input/nlp-getting-started/test.csv\",encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.407035Z","iopub.execute_input":"2021-10-05T16:28:21.407481Z","iopub.status.idle":"2021-10-05T16:28:21.426985Z","shell.execute_reply.started":"2021-10-05T16:28:21.407435Z","shell.execute_reply":"2021-10-05T16:28:21.426019Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# drop columns we don't use","metadata":{}},{"cell_type":"code","source":"df=df.drop(['id','location','keyword'],axis='columns')\ndf.replace(np.nan, 'Miss', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.428293Z","iopub.execute_input":"2021-10-05T16:28:21.428951Z","iopub.status.idle":"2021-10-05T16:28:21.436815Z","shell.execute_reply.started":"2021-10-05T16:28:21.428916Z","shell.execute_reply":"2021-10-05T16:28:21.435909Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"df_test=df_test.drop(['id','location','keyword'],axis='columns')\ndf_test.replace(np.nan, 'Miss', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.439813Z","iopub.execute_input":"2021-10-05T16:28:21.440082Z","iopub.status.idle":"2021-10-05T16:28:21.449957Z","shell.execute_reply.started":"2021-10-05T16:28:21.440045Z","shell.execute_reply":"2021-10-05T16:28:21.448768Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"# Applying pre-processing techniques","metadata":{}},{"cell_type":"code","source":"def pre_processing(tweets,boolean):\n    dataCaracteres = []\n    for linha in tweets:\n        linha[0]=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', linha[0], flags=re.MULTILINE)\n        linha[0]=re.sub(u'[^a-zA-Z0-9áéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ:; ]', '', linha[0])\n        linha[0]=linha[0].replace(\"\\n\",\"\")\n        linha[0] = re.sub(r'@\\S+','',linha[0])\n        linha[0] = re.sub(r'#\\S+','',linha[0]) \n        linha[0] = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+','',linha[0]) \n        linha[0] = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', linha[0])  \n        linha[0] = re.sub(r'[0-9]', '', linha[0])\n        linha[0] = re.sub(\"[\"\n                            u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\",' ',linha[0])\n        \n        dataCaracteres.append(linha)\n    if boolean:\n        base =pd.DataFrame(dataCaracteres,columns=['text','target'])\n    else:\n        base =pd.DataFrame(dataCaracteres,columns=['text'])\n    base = base.apply(lambda x: x.astype(str).str.lower()).apply(lambda x:x.astype(str).str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8'))\n\n\n    base['text']= [word_tokenize(entry) for entry in base['text']]\n    base['text'].dropna(inplace=True)\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n\n    for index,entry in enumerate(base['text']):\n        Final_words = []\n        word_Lemmatized = WordNetLemmatizer()\n        for word, tag in pos_tag(entry):\n            if word not in stopwords.words('english') and word.isalpha():\n                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n                Final_words.append(word_Final)\n        base.loc[index,'text'] = str(Final_words)\n        \n    return base","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.469901Z","iopub.execute_input":"2021-10-05T16:28:21.470150Z","iopub.status.idle":"2021-10-05T16:28:21.488947Z","shell.execute_reply.started":"2021-10-05T16:28:21.470110Z","shell.execute_reply":"2021-10-05T16:28:21.488113Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"tweets=df.to_numpy()\ntweets_test = df_test.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.490070Z","iopub.execute_input":"2021-10-05T16:28:21.490322Z","iopub.status.idle":"2021-10-05T16:28:21.508440Z","shell.execute_reply.started":"2021-10-05T16:28:21.490294Z","shell.execute_reply":"2021-10-05T16:28:21.507337Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"\ndf_1 = pre_processing(tweets,True)\ndf_1.to_csv('preprocessing.csv', index=False)\ndf_1.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:21.510121Z","iopub.execute_input":"2021-10-05T16:28:21.510462Z","iopub.status.idle":"2021-10-05T16:28:53.283840Z","shell.execute_reply.started":"2021-10-05T16:28:21.510423Z","shell.execute_reply":"2021-10-05T16:28:53.283011Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"## this step is not necessary. I used to not apply pre-processing techniques to each execution","metadata":{}},{"cell_type":"code","source":"\ndf_test = pre_processing(tweets_test,False)\ndf_test.to_csv('preprocessing_test.csv', index=False)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:28:53.285181Z","iopub.execute_input":"2021-10-05T16:28:53.285521Z","iopub.status.idle":"2021-10-05T16:29:05.381268Z","shell.execute_reply.started":"2021-10-05T16:28:53.285480Z","shell.execute_reply":"2021-10-05T16:29:05.380647Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"df_2=pd.read_csv(r\"preprocessing.csv\",encoding='latin-1')\ndf_2_test=pd.read_csv(r\"preprocessing_test.csv\",encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:05.382709Z","iopub.execute_input":"2021-10-05T16:29:05.382980Z","iopub.status.idle":"2021-10-05T16:29:05.410905Z","shell.execute_reply.started":"2021-10-05T16:29:05.382952Z","shell.execute_reply":"2021-10-05T16:29:05.410171Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"markdown","source":"# I tested some techniques to balance the classes, but it didn't get a good performance.","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\ndef under_sampler(X,Y):\n    rus = RandomUnderSampler()\n    return rus.fit_resample(X, Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:05.411915Z","iopub.execute_input":"2021-10-05T16:29:05.412135Z","iopub.status.idle":"2021-10-05T16:29:05.417309Z","shell.execute_reply.started":"2021-10-05T16:29:05.412108Z","shell.execute_reply":"2021-10-05T16:29:05.416349Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"# separating into test and training","metadata":{}},{"cell_type":"code","source":"Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split (df_2['text'], df_2['target'], test_size = 0.20)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:05.418442Z","iopub.execute_input":"2021-10-05T16:29:05.418662Z","iopub.status.idle":"2021-10-05T16:29:05.434515Z","shell.execute_reply.started":"2021-10-05T16:29:05.418635Z","shell.execute_reply":"2021-10-05T16:29:05.433828Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"markdown","source":"# applying tf-idf","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nEncoder = LabelEncoder () \nTrain_Y = Encoder.fit_transform (Train_Y) \nTest_Y = Encoder.fit_transform (Test_Y)\n    \nTfidf_vect = TfidfVectorizer (max_features=10000) \nTfidf_vect.fit(df_2['text'])\nTrain_X_Tfidf = Tfidf_vect.transform (Train_X) \nTest_X_Tfidf = Tfidf_vect.transform (Test_X)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:05.437150Z","iopub.execute_input":"2021-10-05T16:29:05.437843Z","iopub.status.idle":"2021-10-05T16:29:05.776330Z","shell.execute_reply.started":"2021-10-05T16:29:05.437809Z","shell.execute_reply":"2021-10-05T16:29:05.775433Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"# generating our training model and plotting the respective confusion matrix","metadata":{}},{"cell_type":"code","source":"SVM = svm.SVC(class_weight={1: 1.427422806481},C=0.5, kernel='linear', degree=3, gamma='auto')\nSVM.fit(Train_X_Tfidf,Train_Y)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:05.777488Z","iopub.execute_input":"2021-10-05T16:29:05.777700Z","iopub.status.idle":"2021-10-05T16:29:09.183436Z","shell.execute_reply.started":"2021-10-05T16:29:05.777676Z","shell.execute_reply":"2021-10-05T16:29:09.182827Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits = 5, shuffle = True)\ny_pred = cross_val_predict(SVM, Test_X_Tfidf, Test_Y, cv = cv)\nfig, ax = plt.subplots()\nsns.heatmap(confusion_matrix(Test_Y, y_pred), annot=True, \n            ax=ax, fmt='d', cmap='Reds')\nax.set_title(\"Matriz de Confusão\", fontsize=18)\nax.set_ylabel(\"True label\")\nax.set_xlabel(\"Predicted Label\")\n# relatório do modelo\nprint('Relatório de classificação:\\n', classification_report(Test_Y, y_pred, digits=4))","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:09.184356Z","iopub.execute_input":"2021-10-05T16:29:09.185014Z","iopub.status.idle":"2021-10-05T16:29:10.322175Z","shell.execute_reply.started":"2021-10-05T16:29:09.184975Z","shell.execute_reply":"2021-10-05T16:29:10.321213Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":"![](https://i.gyazo.com/f7b31510dd705fe4b10f2766614cc6e3.png)","metadata":{}},{"cell_type":"markdown","source":"# applying k-folds to find average accuracy","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ShuffleSplit\nX=df_2['text']\ny=df_2['target']\n\nbase=pd.DataFrame()\n\n#modelo\nmodelo = svm.SVC(class_weight={1: 1.427422806481},C=0.5, kernel='linear', degree=3, gamma='auto')\n\n# Vamos criar 10 separações, mantendo 20% de teste em cada uma\nrs = ShuffleSplit(n_splits=5, test_size=.20)\n\n# List para armazernamos as acurácias de cada modelo\nacuracias = []\n# Agora iteramos por cada separação e treinamos um modelo\n# O método split retorna índices para o vetor passado como parametro\n# O .loc é a maneira de indexar tabelas no pandas a partir de índices\npredicoes_array=[]\ncount=0\nfor train_index, test_index in rs.split(X):\n      \n    Encoder = LabelEncoder () \n    train_y = Encoder.fit_transform (y.loc[train_index]) \n    test_y = Encoder.fit_transform (y.loc[test_index]) \n    Tfidf_vect = TfidfVectorizer (max_features=10000) \n    Tfidf_vect.fit (df_2['text'])\n    train_x = Tfidf_vect.transform (X.loc[train_index]) \n    test_x= Tfidf_vect.transform (X.loc[test_index])  \n  #print(train_x.sample(n=5))\n  #auxX, auxY = under_sampler(train_x, train_y)\n    auxX=train_x\n    auxY=train_y\n    modelo_treinado = modelo.fit(auxX,auxY)\n    predicoes = modelo_treinado.predict(test_x)\n    base[count] = predicoes\n    acuracias.append(accuracy_score(test_y, predicoes))\n    count=count+1\nprint(acuracias)\nprint(\"Média das acurácias: \", sum(acuracias) / len(acuracias))\n#Uma abordagem seria estratificar a separação entre treino/teste. A estratificação procura garantir que a distribuição dos dados é semelhante entre os grupos. Ela pode ser feita tanto para atributos como para rótulos. \n","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:10.323648Z","iopub.execute_input":"2021-10-05T16:29:10.323998Z","iopub.status.idle":"2021-10-05T16:29:31.005265Z","shell.execute_reply.started":"2021-10-05T16:29:10.323962Z","shell.execute_reply":"2021-10-05T16:29:31.004328Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"# generating our final model and saving the submission document","metadata":{}},{"cell_type":"code","source":"train = df_2.drop('target',axis='columns')\n\nEncoder = LabelEncoder () \nTrain_Y = Encoder.fit_transform (df_2['target']) \n \nTfidf_vect = TfidfVectorizer (max_features=10000) \nTfidf_vect.fit(df_2['text'])\nTrain_X_Tfidf = Tfidf_vect.transform (train['text']) \nTest_X_Tfidf = Tfidf_vect.transform (df_2_test['text'])","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:31.006549Z","iopub.execute_input":"2021-10-05T16:29:31.006775Z","iopub.status.idle":"2021-10-05T16:29:31.413199Z","shell.execute_reply.started":"2021-10-05T16:29:31.006749Z","shell.execute_reply":"2021-10-05T16:29:31.412395Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"modelo_final = svm.SVC(class_weight={1: 1.327422806481},C=0.5, kernel='linear', degree=3, gamma='auto')\nmodelo_final.fit(Train_X_Tfidf,Train_Y)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:31.414816Z","iopub.execute_input":"2021-10-05T16:29:31.415408Z","iopub.status.idle":"2021-10-05T16:29:36.201835Z","shell.execute_reply.started":"2021-10-05T16:29:31.415351Z","shell.execute_reply":"2021-10-05T16:29:36.201199Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"predicoes=SVM.predict(Test_X_Tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:36.203531Z","iopub.execute_input":"2021-10-05T16:29:36.204527Z","iopub.status.idle":"2021-10-05T16:29:37.377938Z","shell.execute_reply.started":"2021-10-05T16:29:36.204472Z","shell.execute_reply":"2021-10-05T16:29:37.377156Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(r'/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:37.379307Z","iopub.execute_input":"2021-10-05T16:29:37.379906Z","iopub.status.idle":"2021-10-05T16:29:37.387109Z","shell.execute_reply.started":"2021-10-05T16:29:37.379872Z","shell.execute_reply":"2021-10-05T16:29:37.386484Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"submission['target'] = predicoes","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:37.388153Z","iopub.execute_input":"2021-10-05T16:29:37.388471Z","iopub.status.idle":"2021-10-05T16:29:37.396384Z","shell.execute_reply.started":"2021-10-05T16:29:37.388442Z","shell.execute_reply":"2021-10-05T16:29:37.395784Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:37.398175Z","iopub.execute_input":"2021-10-05T16:29:37.398622Z","iopub.status.idle":"2021-10-05T16:29:37.416188Z","shell.execute_reply.started":"2021-10-05T16:29:37.398578Z","shell.execute_reply":"2021-10-05T16:29:37.415186Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('./submission.csv', index = False)\nprint('./submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-05T16:29:37.417601Z","iopub.execute_input":"2021-10-05T16:29:37.417909Z","iopub.status.idle":"2021-10-05T16:29:37.434693Z","shell.execute_reply.started":"2021-10-05T16:29:37.417881Z","shell.execute_reply":"2021-10-05T16:29:37.433779Z"},"trusted":true},"execution_count":97,"outputs":[]}]}